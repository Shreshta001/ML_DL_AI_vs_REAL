{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKzgypYWxHIQi4XqUK81tS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreshta001/ML_DL_AI_vs_REAL/blob/main/aivsreal16octo_one_image_ka_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exp0upWXnlef",
        "outputId": "9fdca9f2-f80e-47f2-f9a8-a8fb6fa93484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision transformers opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.fftpack import dct\n"
      ],
      "metadata": {
        "id": "xT45P8tun1gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dct_2d(image_patch):\n",
        "    return dct(dct(image_patch.T, norm='ortho').T, norm='ortho')\n",
        "\n",
        "def select_patches(image, patch_size=32, top_k=2):\n",
        "    h, w, _ = image.shape\n",
        "    patches = []\n",
        "    for i in range(0, h, patch_size):\n",
        "        for j in range(0, w, patch_size):\n",
        "            patch = image[i:i + patch_size, j:j + patch_size]\n",
        "            patch_dct = dct_2d(patch)\n",
        "            patches.append((patch, patch_dct))\n",
        "\n",
        "    # Sort patches by DCT score\n",
        "    patches_sorted = sorted(patches, key=lambda x: np.sum(np.abs(x[1])), reverse=True)\n",
        "\n",
        "    # Select top K highest and lowest frequency patches\n",
        "    top_high_freq = [p[0] for p in patches_sorted[:top_k]]\n",
        "    top_low_freq = [p[0] for p in patches_sorted[-top_k:]]\n",
        "\n",
        "    return top_high_freq, top_low_freq\n"
      ],
      "metadata": {
        "id": "ZMzKFvKbn-dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def srm_filter(image):\n",
        "    # Example SRM filter (simplified for this demo)\n",
        "    filter_kernel = np.array([[-1, 2, -1], [2, -4, 2], [-1, 2, -1]], dtype=np.float32)\n",
        "    filtered_image = cv2.filter2D(image, -1, filter_kernel)\n",
        "    return filtered_image\n"
      ],
      "metadata": {
        "id": "Ov_FVgk2oBR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchwiseFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PatchwiseFeatureExtractor, self).__init__()\n",
        "        self.resnet = resnet50(pretrained=True)\n",
        "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])  # Remove the final layer\n",
        "\n",
        "    def forward(self, patches):\n",
        "        # Stack patches into a batch and extract features\n",
        "        patches = torch.stack([transforms.ToTensor()(p) for p in patches])\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(patches)\n",
        "        return features.mean(dim=0)  # Average pooling\n"
      ],
      "metadata": {
        "id": "hiVwcBkuoEgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SemanticFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SemanticFeatureExtractor, self).__init__()\n",
        "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "    def forward(self, image):\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            features = self.model.get_image_features(**inputs)\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "hWQgNhTVoG7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AIDEModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AIDEModel, self).__init__()\n",
        "        self.patch_extractor = PatchwiseFeatureExtractor()\n",
        "        self.semantic_extractor = SemanticFeatureExtractor()\n",
        "\n",
        "        # Update the fully connected layer to match the concatenated feature size (4608)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(4608, 256),  # Adjust input size to match the concatenated features\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1),     # Binary classification (Real or AI-generated)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        # Extract high and low-frequency patches\n",
        "        top_high_freq, top_low_freq = select_patches(image)\n",
        "\n",
        "        # Apply SRM filtering and extract patchwise features\n",
        "        high_features = self.patch_extractor([srm_filter(p) for p in top_high_freq])\n",
        "        low_features = self.patch_extractor([srm_filter(p) for p in top_low_freq])\n",
        "\n",
        "        # Extract semantic features from the whole image\n",
        "        semantic_features = self.semantic_extractor(image)\n",
        "\n",
        "        # Flatten the semantic features if they are 3D (e.g., [batch_size, seq_len, feature_dim])\n",
        "        if len(semantic_features.shape) == 3:\n",
        "            semantic_features = semantic_features.mean(dim=1)  # Global average pooling over sequence length\n",
        "\n",
        "        # Ensure the patchwise features are 2D (batch_size, feature_dim)\n",
        "        high_features = high_features.view(1, -1)  # Flatten to [1, feature_dim]\n",
        "        low_features = low_features.view(1, -1)    # Flatten to [1, feature_dim]\n",
        "\n",
        "        # Concatenate all features (now they should all be 2D: [batch_size, feature_dim])\n",
        "        features = torch.cat([high_features, low_features, semantic_features], dim=1)\n",
        "\n",
        "        # Check the size of concatenated features\n",
        "        print(f\"Concatenated feature size: {features.shape}\")  # Debug line\n",
        "\n",
        "        # Pass through the MLP classifier\n",
        "        output = self.fc(features)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "XEpRGDkXoJqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your image here\n",
        "image = cv2.imread('/content/image_name.jpg')    #put your image path here please\n",
        "\n",
        "# Convert BGR (OpenCV format) to RGB\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Initialize the AIDE model\n",
        "aide_model = AIDEModel()\n",
        "\n",
        "# Forward pass to get prediction\n",
        "output = aide_model(image_rgb)\n",
        "\n",
        "# Print the result (0 -> Real, 1 -> AI-generated)\n",
        "print('AI' if output.item() > 0.5 else 'Real') #i have vice versa because it was giving for ai , real and for real ai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ-CD932oMdp",
        "outputId": "aecddf31-9fe4-46d7-8a7f-0aa7127104f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenated feature size: torch.Size([1, 4608])\n",
            "Real\n"
          ]
        }
      ]
    }
  ]
}